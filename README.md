# Data Governance Lakehouse Demo

A production-inspired **end-to-end data engineering template project** demonstrating how to design, build, and operate a modern **Lakehouse architecture** aligned with **DAMA-DMBOK (Data Management Body of Knowledge)** Data Governance principles.

This repository is designed as a **portfolio-grade reference implementation** for Data Engineers, Analytics Engineers, and Platform Engineers.

---

## ğŸ¯ Project Goals

This project demonstrates:

* Practical implementation of **Data Governance domains** (DAMA-DMBOK)
* Modern **Lakehouse architecture**
* Production-style **data ingestion, transformation, orchestration, quality checks, lineage, and security**
* Realistic enterprise data workflows

---

## ğŸ— Architecture Overview

```
API Sources â†’ Airflow (ETL/ELT)
           â†’ Raw Zone (S3)
           â†’ PostgreSQL DWH
               â”œâ”€â”€ ODS Layer
               â”œâ”€â”€ Staging Layer
               â””â”€â”€ Data Mart Layer

PostgreSQL â†’ Metabase (BI)

Monitoring:
- Great Expectations (Data Quality)
- OpenMetadata (Metadata & Lineage)
```

---

## ğŸ“š DAMA-DMBOK Mapping

This project explicitly maps features to **Data Governance knowledge areas**.

| DAMA Domain               | Implementation                   |
| ------------------------- | -------------------------------- |
| Data Architecture         | Lakehouse design, layered DWH    |
| Data Modeling & Design    | Dimensional models, star schemas |
| Data Storage & Operations | S3 + PostgreSQL + partitioning   |
| Data Security             | RBAC, secrets management         |
| Data Integration          | API ingestion, batch pipelines   |
| Documents & Content       | Data contracts, schema docs      |
| Reference & Master Data   | Lookup tables, dimensions        |
| Data Warehousing & BI     | Data marts, Metabase             |
| Metadata                  | OpenMetadata catalog             |
| Data Quality              | Great Expectations checks        |

---

## ğŸ§© Tech Stack

### Core

* **Python 3.11**
* **Apache Airflow** â€” orchestration
* **PostgreSQL** â€” analytical warehouse
* **AWS S3 / MinIO** â€” data lake storage
* **dbt** â€” transformations

### Governance Layer

* **Great Expectations** â€” data quality
* **OpenMetadata** â€” metadata, lineage

### BI

* **Metabase** â€” analytics and dashboards

### Infrastructure

* **Docker & Docker Compose**
* **Makefile automation**

---

## ğŸ“‚ Repository Structure

```
project-root/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ ods/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ lakehouse/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”‚
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ openmetadata/
â”‚
â”œâ”€â”€ quality/
â”‚   â””â”€â”€ great_expectations/
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ env/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ governance.md
â”‚   â””â”€â”€ data_dictionary.md
â”‚
â””â”€â”€ README.md
```

---

## ğŸ”„ Data Pipeline Flow

### Step 1 â€” Data Ingestion

* Pull data from public API (example: Earthquake API)
* Store raw JSON into S3 (Raw Zone)

### Step 2 â€” Load to ODS

* Airflow loads raw data into PostgreSQL ODS schema

### Step 3 â€” Transformation (dbt)

* Staging models: cleaning & typing
* ODS models: normalized layer
* Data Marts: business-ready tables

### Step 4 â€” Data Quality

* Validate:

  * Schema
  * Null constraints
  * Freshness
  * Volume anomalies

### Step 5 â€” Metadata & Lineage

* Automatically register datasets
* Track pipeline lineage

### Step 6 â€” BI Consumption

* Metabase dashboards built on Gold layer

---

## ğŸ“Š Data Governance Implementation

### Data Quality

* Expectations:

  * Not null
  * Unique keys
  * Valid ranges
  * Referential integrity

### Metadata Management

* Dataset ownership
* Column descriptions
* Business glossary

### Security

* Separate service users
* Read-only BI role
* Environment secrets

### Documentation

* Architecture diagrams
* Data dictionary
* Model descriptions

---

## ğŸš€ How To Run Locally

### Prerequisites

* Docker
* Docker Compose
* Make

### Start Stack

```bash
make up
```

Services:

| Service      | URL                                            |
| ------------ | ---------------------------------------------- |
| Airflow      | [http://localhost:8080](http://localhost:8080) |
| Metabase     | [http://localhost:3000](http://localhost:3000) |
| OpenMetadata | [http://localhost:8585](http://localhost:8585) |
| Postgres     | localhost:5432                                 |

---

## ğŸ§ª Example Use Cases

* Portfolio project for Data Engineers
* Interview demo project
* Governance framework reference
* Training sandbox

---

## ğŸ“ˆ Future Extensions

* CDC streaming (Kafka)
* Iceberg tables
* Feature Store
* ML pipelines
* Data mesh domains

---

## ğŸ“œ License

MIT License

---

## â­ Why This Project Matters

Most demo projects show only ingestion or dashboards.

This project shows:

* Enterprise-grade architecture
* Governance-first mindset
* Production-style workflows

It reflects how modern data platforms are built in real companies.
